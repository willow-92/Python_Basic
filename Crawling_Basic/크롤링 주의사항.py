# 웹 크롤링 및 스크래핑 전 주의사항
# 사전 기초 지식
# 대상 웹 페이지 조건 확인 - robots.txt
# 크롤러 분류 - 상태 유무, Javascript 유무
# Request 요청 주의 할 점 - 서버 부하 고려
# 콘텐츠 저작권 문제
# 페이지 구조 변경 가능성 숙지

# 대상 웹 페이지 조건 확인 - robots.txt
# robots.txt 는 네이버나 다음에도 다 있음. 
# 다음에 daum.net/robots.txt를 치면 
# User-agent: *
# Disallow: /
# 원칙적으로 다음 사이트는 크롤링을 허용하지 않고 있음

# 또 다른 예시로, 한 빛 미디어
# 한빛에서 나오는 책들에 대한 정보를 분석하는 미션이 있다면
# 로봇 티엑스티를 보면
# Disallow는 /hb.admin/ 과 /myhanbit/
# 회원가입에서 로그인 후 나오는 정보.
# 가입 하지 않은 상태에서 이 두개의 폴더를 제외하고는 크롤링을 허용한다고 볼 수 있음
# 아까 다음이 그랬는데, 유저 얼라우가 / 로 되어 있으면 해당 내용은 모든 로봇에 대한 접근을 차단한다는 것
# 참고로 로봇 txt라고 하면 하나의 설명을 잘 해 놓은 블로그를 참고하면 됨
# 얼라우 라고 되어 있으면 모든 로봇이 접근이 가능하다는 것
# 단 부하를 일으키면 안된다. 
# 한빛 같은 경우에 disallow의 경우 2개의 경로를 제외하고서는, 부하를 일으키지 않을 떄는 허용
# 특정 봇. 구글 봇에 대해서는 허락하지 않는다.
# 구글 봇 로봇만 허용 등 사이트마다 로봇 티엑스티를 두고 봇들이 방문을 해서 허용이 되어 있기 때문에 구글 봇이 한빛 사이트의 내용을 크롤링 했기 때문에 
# 검색하면 한빛사이트 내용이 나오는 것. 무조건 허용하지 않는 것이 중요한 것이 아님
# 사람들에게 많이 보여주고 싶은 부분은 얼라우를 해줘야 구글에서 검색이 된다.
# 구글 봇이 가서 크롤링을 해와야 검색을 쳤을 때 랭킹을 가려서 품질이 좋은 글이 상위에 올라가게 되어 있고
# 상위에 있는 글들이 마케팅적으로 비싸고. 나에게 이익이 될 수 있는 정보이기 때문
# 조건을 확인해야 함


# 크롤러 분류 - 상태가 있는지? 
# 내가 가져올 정보가 반드시 로그인을 해야 보여지는 것들이 있음.
# 네이버 카페에 특정 게시판의 경우 회원가입해야 볼 수 있는데, 로그인을 하지 않아도 볼 수 있는 사이트. 엔카 같은 곳.
# 여기는 로그인을 하지 않고도 자동차 판매 정보를 볼 수 있음
# 로그인, 어떤 상태가 있는지를 봐야 하고.
# 자바스크립트를 사용해서 정보를 보는 곳은 셀레늄이나 다른 엔진을 사용해야 함
# 그래서 내가 가져올 정보의 상태와 자바스크립트 사용 유무를 확인해야 한다. 
# 지금은 어렵다. ajax라고 불리는 비동기식으로 데이터를 렌더링 해주는 사이트는 나중에 셀레늄이라는 파이썬의 스크래핑할 때 쓰는 엔진을 활용하면 쉽게 가져올 수 있다.

# 가장 중요한 것
# 리퀘스트 요청할 때 주의할 점 - 서버 부하 고려
# 크롤러를 만들었는데, 네이버에서 신문기사를 1초에 10개씩, 1초에 100개씩 가져오는 것을 만들었다고 친다면, 서버 관리자가 알게 된다.
# 그럼 문제의 소지가 될 수 있기 때문에, 서버의 부하를 고려하고, 다른 사용자가 피해를 입으면 안된다. 
# 그랬을 때는 불미스러운 일이 있기 때문에 타이밍을 조정해서 사람이 하는 것 처럼 느긋하게 보는데, 클릭하면 기다렸다가 슥 보고. 그 다음에 다시 클릭을 해서 보고. 
# 눈으로 읽는다고 보고. 이 정도 속도라면 사람이 보는 거랑 비슷하다고 볼 수 있음. 이런 것들은 서버의 부하를 고려하지 않고 느슨하게 타이밍을 줘서 데이터를 수집할 수 있다.
# 하지만 빠른 시간 내에 for문 같은 것을 활용하면 아이피가 벤을 당한다던지, 집에서 특정 사이트가 막히는 경우가 있을 수 있다. 
# 그러면 전화해서 죄송하다는 이야기를 드리고 아이피 차단을 해제한 적이 있다.
# 가장 중요한 것은 상대 사이트에 대한 예의를 지켜야 한다.
# 예의를 지킨다는 것은, 컴퓨터는 엄청 빠르기 때문에 충분한 간격을 두고 보내야 한다는 것. 

# 콘텐츠 저작권 문제
# 텍스트만 크롤링 하는 것이 아니라 이미지도 크롤링을 하게 된다.
# 이런 이미지들을 수집을 해서, 또는 마음대로 활용할 때, 결과물을 웹에 다시 올리거나 할 떄 컨텐츠 저작권 문제에 휘말릴 수 있다. 
# 반드시 이미지의 저작권을 확인하고 상업적이나 개인적 스터디 용도로 사용해도, 블로그에 공개한다던지, 대중에게 공개한다던지 할 때, 어떤 라이센스를 확인해야 한다. 
# 특정 사이트의 문서, 크롤링 해왔는데 유죄가 되는 경우도 있다. 논문 같은 것. 

# 구조 변경 가능성 숙지
# 오늘 날을 새서 어떤 크롤러를 만들었는데, 매일 아침 9시에 현재 급상승 검색어 1위~10위 까지를 가져오는 것을 만들었는데,
# 내일도 바뀌고 한 시간 뒤도 바뀔텐데, 어느 날 자동화 시켜놨는데, 만들어 놓은 코드가 안되는 걸 보니까 네이버가 위치를 옮긴 것.
# 포털 사이트들은 디자인도 자주 바뀌고 위치도 자주 바뀐다. 
# 그렇기 때문에 셀렉터 같은 것들이 바뀌기 때문에 잘 되던 코드가 안되면 개발자도구를 열어서 가져오고 싶은 것을 클릭해서 소스코드를 카피해서 바꿔줘야 한다. 
# 심할 경우 엔진을 교체해야 하는 경우가 생길 수도 있음. 
# 결론: 예제 파일을 따라할 때, 수업을 듣는 시점에 신문기사 부분이 아래로 내려갔을 수도 있고 없어졌을 수도 있음.
# 이런 것들은 반드시 선택자를 활용해서 선택자 부분을 조정해서 코딩을 해야 한다. 
# 내가 잘 만들어 놓은게 안되면 소스코드를 수정하는 작업이 필요함
# 하루에 한 번 씩 바뀌는 경우도 있음. 

# 리퀘스트 관련해서 요즘에는 API를 제공하고 있음
# 이 API를 통해서 유튜브에 있는 동영상 정보나 조회스 리플 들, 좋아요 등을 제공하고 있음. 
# 포털 사이트들은, 데이터 API가 있다. 이런 API를 활용하면 서버 부하를 주지 않을 수 있음. 
# 부득이한 경우 로봇 티엑스티를 활용해서 서버 부하를 주지 않는 형태로.
# 이미지, 동영상, 소리, 음원 파일 등의 저작권 문제를 반드시 고려해야 한다. 
